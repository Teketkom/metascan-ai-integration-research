# üèó –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Metascan

## Executive Summary

**Metascan** –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π **distributed microservices-based cloud platform** –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª—è:

- ‚ö° **High Throughput**: 500K+ –∞—Å—Å–µ—Ç–æ–≤/–¥–µ–Ω—å
- üìè **Horizontal Scaling**: 300+ compute nodes
- üîÑ **Event-Driven**: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- üõ°Ô∏è **Fault-Tolerant**: Auto-recovery, circuit breakers
- üìä **Observable**: Full-stack monitoring

---

## 1. High-Level –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### 1.1 System Overview

```mermaid
graph TB
    subgraph "Client Layer"
        A[–í–µ–± UI] --> B[API Gateway]
        C[CLI Tool] --> B
        D[External APIs] --> B
    end
    
    subgraph "Application Layer"
        B --> E[Auth Service]
        B --> F[Asset Management]
        B --> G[Scan Orchestrator]
        B --> H[Report Engine]
    end
    
    subgraph "Scanning Layer - 29 Engines"
        G --> I[Network Scanners]
        G --> J[Web Scanners]
        G --> K[Auth Bruteforce]
        G --> L[CVE Detection]
    end
    
    subgraph "Data Layer"
        I --> M[(PostgreSQL)]
        J --> M
        K --> M
        L --> M
        M --> N[(ClickHouse Analytics)]
        M --> O[S3 Object Storage]
    end
    
    subgraph "Message Queue"
        G --> P[RabbitMQ / Kafka]
        P --> I
        P --> J
    end
```

### 1.2 –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

#### Frontend Layer

- **Web UI**: React/Vue.js SPA
- **API Gateway**: Kong / NGINX (rate limiting, auth)
- **CLI Tool**: Python-based scanner client

#### Application Services

| Service | –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|---------|-----------|----------|
| **Auth Service** | Go + JWT | –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è, RBAC |
| **Asset Manager** | Python/Django | –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ IP/–¥–æ–º–µ–Ω–∞–º–∏ |
| **Scan Orchestrator** | Python/Celery | –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è —Å–∫–∞–Ω–æ–≤ |
| **Report Engine** | Python + Jinja2 | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF/HTML –æ—Ç—á–µ—Ç–æ–≤ |
| **Notification Service** | Go | Email/Slack/Telegram –∞–ª–µ—Ä—Ç—ã |

#### Scanning Engines (29 –º–æ–¥—É–ª–µ–π)

**Network Layer**:
- `nmap` - Port scanning (0-65535)
- `masscan` - High-speed SYN scanning
- `amass` - Subdomain enumeration
- `dnsx` - DNS resolution

**Web Application Layer**:
- `nuclei` - Template-based CVE detection
- `katana` - Web crawler
- `httpx` - HTTP probing
- `ZAP (OWASP)` - DAST scanning
- `dirsearch` - Directory bruteforce
- `wafw00f` - WAF fingerprinting
- `magescan` - CMS vulnerability detection

**Authentication Layer**:
- `hydra` - Login bruteforce (SSH, FTP, MySQL, etc.)
- Custom brute-force modules

**Vulnerability Detection**:
- `cve-search` - CVE matching
- `vulners` - Vulnerability intelligence
- Custom Python/Go detectors

### 1.3 Data Flow

```mermaid
sequenceDiagram
    participant U as User/API
    participant O as Orchestrator
    participant Q as Queue (RabbitMQ)
    participant S as Scanner Workers
    participant DB as PostgreSQL
    participant CH as ClickHouse
    participant S3 as Object Storage
    
    U->>O: Submit scan job (domains/IPs)
    O->>DB: Store job metadata
    O->>Q: Publish scan tasks
    
    loop –î–ª—è –∫–∞–∂–¥–æ–≥–æ scanner
        Q->>S: Consume task
        S->>S: Execute scan
        S->>DB: Store raw results
        S->>S3: Upload logs/screenshots
        S->>Q: Publish completion event
    end
    
    Q->>O: All scans complete
    O->>DB: Aggregate results
    O->>CH: ETL for analytics
    O->>U: Notify completion
```

---

## 2. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

### 2.1 Backend

#### –Ø–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è

```python
# Core Services Distribution
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
60% - Python 3.10+
     ‚Ä¢ Django / FastAPI
     ‚Ä¢ Celery workers
     ‚Ä¢ Scanner integrations
     
30% - Go 1.21+
     ‚Ä¢ High-performance services
     ‚Ä¢ Network scanners
     ‚Ä¢ Real-time processing
     
10% - C/C++
     ‚Ä¢ Custom probes
     ‚Ä¢ Performance-critical modules
```

#### Frameworks & Libraries

**Python**:
- `Django 4.x` - Web framework
- `FastAPI` - High-performance APIs
- `Celery` - Distributed task queue
- `SQLAlchemy` - ORM
- `Pydantic` - Data validation
- `aiohttp` - Async HTTP client

**Go**:
- `Gin` - Web framework
- `GORM` - ORM
- `go-redis` - Redis client
- `nats.go` - Message broker client

### 2.2 Infrastructure

#### Cloud Provider: Yandex Cloud

**Compute**:
- **Compute Cloud**: 300+ VMs (s2.medium, s2.large)
- **Instance Groups**: Auto-scaling (10-500 instances)
- **Serverless Containers**: –õ–µ–≥–∫–∏–µ –∑–∞–¥–∞—á–∏

**Storage**:
- **Object Storage (S3)**: –õ–æ–≥–∏, —Å–∫—Ä–∏–Ω—à–æ—Ç—ã, raw data
- **Managed PostgreSQL**: Transactional DB (HA cluster)
- **Managed ClickHouse**: Analytics, OLAP

**Networking**:
- **Virtual Private Cloud**: –ò–∑–æ–ª—è—Ü–∏—è —Å–µ—Ç–µ–π
- **Load Balancer**: Application LB + Network LB
- **Cloud DNS**: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞–º–∏

**Message Brokers**:
- **RabbitMQ** (—Ç–µ–∫—É—â–µ–µ) –∏–ª–∏ **Managed Kafka**
- Redis Pub/Sub –¥–ª—è real-time events

### 2.3 Orchestration & CI/CD

#### Kubernetes

```yaml
# Cluster Configuration
apiVersion: v1
kind: Cluster
metadata:
  name: metascan-prod
spec:
  nodes:
    - type: master
      count: 3
      flavor: s2.medium (4 vCPU, 16GB RAM)
    - type: worker
      count: 50+
      flavor: s2.large (8 vCPU, 32GB RAM)
      autoscaling:
        min: 10
        max: 500
  
  namespaces:
    - production
    - staging
    - development
```

**–ö–ª—é—á–µ–≤—ã–µ workloads**:
- `Deployments` - API services
- `StatefulSets` - Databases, message brokers
- `DaemonSets` - Monitoring agents
- `Jobs/CronJobs` - Scheduled scans

#### CI/CD Pipeline

```mermaid
graph LR
    A[Git Push] --> B[GitLab CI]
    B --> C[Build Docker]
    B --> D[Run Tests]
    D --> E[Security Scan]
    E --> F[Push to Registry]
    F --> G[Deploy to Staging]
    G --> H[E2E Tests]
    H --> I[Deploy to Prod]
    I --> J[Smoke Tests]
```

**Tools**:
- **GitLab CI** / GitHub Actions
- **Docker Registry**: Yandex Container Registry
- **Helm**: K8s package manager
- **ArgoCD**: GitOps deployments

---

## 3. –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞

### 3.1 Scan Workflow

```python
# Orchestrator Pseudocode

def orchestrate_scan(asset_list):
    """
    –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Å–∫–∞–Ω–∞
    """
    # Phase 1: Discovery
    domains = enumerate_subdomains(asset_list)
    ips = resolve_dns(domains)
    
    # Phase 2: Port Scanning (parallel)
    open_ports = scan_ports(ips, range(0, 65536))
    
    # Phase 3: Service Detection
    services = fingerprint_services(open_ports)
    
    # Phase 4: Vulnerability Detection
    vulnerabilities = []
    for service in services:
        # –ó–∞–ø—É—Å–∫ 29 –¥–≤–∏–∂–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [
            nuclei.scan(service),
            zap.scan(service),
            nmap_vuln_scripts(service),
            custom_detectors(service)
        ]
        results = await asyncio.gather(*tasks)
        vulnerabilities.extend(results)
    
    # Phase 5: Validation & Deduplication
    validated = validate_vulnerabilities(vulnerabilities)
    deduplicated = remove_duplicates(validated)
    
    # Phase 6: Prioritization
    prioritized = calculate_risk_scores(deduplicated)
    
    return prioritized
```

### 3.2 Parallelization Strategy

**–£—Ä–æ–≤–Ω–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞**:

1. **–£—Ä–æ–≤–µ–Ω—å –∞—Å—Å–µ—Ç–æ–≤**: 500K –∞—Å—Å–µ—Ç–æ–≤ ‚Üí 500 workers
2. **–£—Ä–æ–≤–µ–Ω—å —Å–∫–∞–Ω–µ—Ä–æ–≤**: 29 –¥–≤–∏–∂–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
3. **–£—Ä–æ–≤–µ–Ω—å –ø–æ—Ä—Ç–æ–≤**: –ë–∞—Ç—á–∏ –ø–æ 1K –ø–æ—Ä—Ç–æ–≤

**Concurrency Control**:
- Rate limiting: –ò–∑–±–µ–∂–∞–Ω–∏–µ DDoS –Ω–∞ target
- Resource limits: CPU/Memory quotas per pod
- Backpressure: Queue depth monitoring

### 3.3 –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ

#### Data Schema (PostgreSQL)

```sql
-- –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã

CREATE TABLE assets (
    id UUID PRIMARY KEY,
    type VARCHAR(20), -- domain, ip, subnet
    value TEXT,
    owner_id UUID,
    created_at TIMESTAMP
);

CREATE TABLE scan_jobs (
    id UUID PRIMARY KEY,
    asset_ids UUID[],
    status VARCHAR(20),
    started_at TIMESTAMP,
    completed_at TIMESTAMP
);

CREATE TABLE vulnerabilities (
    id UUID PRIMARY KEY,
    asset_id UUID REFERENCES assets(id),
    scan_job_id UUID REFERENCES scan_jobs(id),
    cve_id VARCHAR(20),
    cvss_score DECIMAL(3,1),
    severity VARCHAR(10),
    title TEXT,
    description TEXT,
    proof_of_concept TEXT,
    remediation TEXT,
    discovered_at TIMESTAMP,
    validated BOOLEAN DEFAULT FALSE
);

CREATE TABLE scan_results_raw (
    id BIGSERIAL PRIMARY KEY,
    scan_job_id UUID,
    scanner_name VARCHAR(50),
    asset_id UUID,
    raw_output JSONB,
    created_at TIMESTAMP
);
```

#### Analytics (ClickHouse)

```sql
-- OLAP –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –∞–≥—Ä–µ–≥–∞—Ü–∏–π

CREATE TABLE vulnerability_metrics (
    date Date,
    customer_id UUID,
    asset_id UUID,
    cve_id String,
    severity LowCardinality(String),
    cvss_score Float32,
    scanner_name LowCardinality(String),
    false_positive UInt8
)
ENGINE = MergeTree()
PARTITION BY toYYYYMM(date)
ORDER BY (date, customer_id, asset_id);
```

---

## 4. Observability & Monitoring

### 4.1 Monitoring Stack

```yaml
# Prometheus + Grafana + Loki + Jaeger

Components:
  - Prometheus:
      - Metrics scraping (15s interval)
      - AlertManager integration
      - 30-day retention
  
  - Grafana:
      - Dashboards:
        * System metrics (CPU, RAM, Disk)
        * Application metrics (RPS, latency, errors)
        * Business metrics (scans/day, vulns detected)
      - Alerts: Slack, PagerDuty
  
  - Loki:
      - Centralized log aggregation
      - Log retention: 90 days
  
  - Jaeger:
      - Distributed tracing
      - Span retention: 7 days
```

### 4.2 –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

#### SLIs (Service Level Indicators)

| –ú–µ—Ç—Ä–∏–∫–∞ | Target | Alert Threshold |
|---------|--------|----------------|
| **API Availability** | 99.9% | <99.5% |
| **API Latency (p95)** | <500ms | >1s |
| **Scan Completion Rate** | >95% | <90% |
| **False Positive Rate** | <15% | >25% |
| **Scanner Uptime** | >99% | <95% |

#### Business Metrics

- **Daily scans**: 500K+ targets
- **Vulns detected/day**: 10K-50K
- **High/Critical alerts**: <1% total
- **Average scan duration**: 4-6 hours

---

## 5. Security –∏ Compliance

### 5.1 Security Architecture

**Network Security**:
- VPC isolation (dev/staging/prod)
- Security Groups (least privilege)
- WAF (Web Application Firewall)
- DDoS protection (Yandex Cloud Shield)

**Application Security**:
- RBAC (Role-Based Access Control)
- JWT authentication + refresh tokens
- API rate limiting (100 req/min per user)
- Input validation (Pydantic, OWASP guidelines)
- HTTPS everywhere (TLS 1.3)

**Data Security**:
- Encryption at rest (AES-256)
- Encryption in transit (TLS 1.3)
- Database credentials in Vault
- Sensitive data masking in logs

### 5.2 Compliance

- **–†–û–° –ü–û**: –†–µ–µ—Å—Ç—Ä ‚Ññ19437
- **–ê–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏—è IT**: –í—ã–ø–∏—Å–∫–∞ –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞
- **–§–ó-152** (–ü–î–Ω): –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –†–ö–ù
- **ISO 27001** (in progress): ISMS certification

---

## 6. Scalability & Performance

### 6.1 –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ —Å–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

**Auto-scaling –ø–æ–ª–∏—Ç–∏–∫–∏**:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: scanner-worker
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: scanner-worker
  minReplicas: 10
  maxReplicas: 500
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### 6.2 Performance –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

**Caching**:
- Redis: API responses, session data
- CDN: Static assets (CloudFlare / Yandex CDN)
- Application-level: LRU cache –¥–ª—è CVE lookups

**Database**:
- Connection pooling (pgbouncer)
- Read replicas (PostgreSQL HA)
- Partitioning (ClickHouse)
- Indexing strategy (B-tree, GiST)

**Network**:
- HTTP/2 + gRPC
- Connection reuse
- Compression (gzip, brotli)

---

## 7. Disaster Recovery

### 7.1 Backup Strategy

**Databases**:
- PostgreSQL: Continuous WAL archiving + daily full backups
- ClickHouse: Daily snapshots to S3
- Retention: 30 days

**Object Storage**:
- S3 versioning enabled
- Cross-region replication (backup datacenter)

**Configuration**:
- GitOps: All configs in Git
- Helm charts versioned
- Secret management: Vault snapshots

### 7.2 RTO/RPO

| Component | RTO | RPO |
|-----------|-----|-----|
| **API Services** | <15 min | 0 (stateless) |
| **PostgreSQL** | <1 hour | <5 min |
| **ClickHouse** | <2 hours | <1 hour |
| **Object Storage** | <30 min | 0 (versioned) |

---

## 8. –í—ã–≤–æ–¥—ã

### –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

‚úÖ **Microservices**: –ù–µ–∑–∞–≤–∏—Å–∏–º–æ–µ —Å–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤  
‚úÖ **Event-Driven**: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, high throughput  
‚úÖ **Cloud-Native**: K8s, auto-scaling, serverless  
‚úÖ **Observable**: Full-stack monitoring  
‚úÖ **Fault-Tolerant**: Circuit breakers, retries  

### –¢–æ—á–∫–∏ —Ä–æ—Å—Ç–∞ –¥–ª—è AI

üî¥ **Data Pipeline**: –ù–µ—Ç ETL –¥–ª—è ML training  
üî¥ **Feature Store**: –ù–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ feature management  
üî¥ **Model Serving**: –ù–µ—Ç inference –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã  
üî¥ **Vector DB**: –ù–µ—Ç similarity search –¥–ª—è CVE embeddings  

**‚Üí –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è AI —Å–ª–æ—è!**

---

**Next**: [üìä Big Data –æ—Ü–µ–Ω–∫–∞](03-big-data-assessment.md)
